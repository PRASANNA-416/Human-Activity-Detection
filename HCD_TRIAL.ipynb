{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNxfgUYwZko336ZWLrXRSW2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PRASANNA-416/Human-Activity-Detection-/blob/main/HCD_TRIAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfyHr_3Z0bPs",
        "outputId": "d299de2c-f71a-49dc-bbe4-ff8cf4e504d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the training CSV file\n",
        "train_csv_path = '/content/gdrive/MyDrive/ML/Human Action Recognition/Human Action Recognition/Final_training_set.csv'\n",
        "train_data = pd.read_csv(train_csv_path)\n",
        "\n",
        "# Load the testing CSV file\n",
        "test_csv_path = '/content/gdrive/MyDrive/ML/Human Action Recognition/Human Action Recognition/Testing_set.csv'\n",
        "test_data = pd.read_csv(test_csv_path)"
      ],
      "metadata": {
        "id": "K_j3AieH0f1G"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "\n",
        "# Define the target image size and batch size\n",
        "target_size = (224, 224)  # Adjust according to your requirements\n",
        "batch_size = 64\n",
        "\n",
        "# Create ImageDataGenerators with preprocessing options\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255.0,  # Normalize pixel values between 0 and 1\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
        "\n",
        "# Create train and test generators\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    dataframe=train_data,\n",
        "    directory='/content/gdrive/MyDrive/ML/Human Action Recognition/Human Action Recognition/FINAL_TRAIN',  # Directory containing the train images\n",
        "    x_col='filename',  # Column name for the filenames\n",
        "    y_col='label',  # Column name for the labels\n",
        "    target_size=target_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_dataframe(\n",
        "    dataframe=test_data,\n",
        "    directory='/content/gdrive/MyDrive/ML/Human Action Recognition/Human Action Recognition/test',\n",
        "    x_col='filename',  # Column name for the filenames\n",
        "    y_col='label',  # Column name for the labels\n",
        "    target_size=target_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Check the class indices assigned by the generator\n",
        "class_indices = test_generator.class_indices\n",
        "print(class_indices)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBBD-e1Z0f3P",
        "outputId": "16f36687-e653-49d1-839f-3854550ca39d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 17566 validated image filenames belonging to 15 classes.\n",
            "Found 2520 validated image filenames belonging to 15 classes.\n",
            "{'calling': 0, 'clapping': 1, 'cycling': 2, 'dancing': 3, 'drinking': 4, 'eating': 5, 'fighting': 6, 'hugging': 7, 'laughing': 8, 'listening_to_music': 9, 'running': 10, 'sitting': 11, 'sleeping': 12, 'texting': 13, 'using_laptop': 14}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# Define the model\n",
        "m = tf.keras.Sequential([\n",
        "    hub.KerasLayer(\n",
        "        'https://tfhub.dev/google/imagenet/mobilenet_v3_small_075_224/feature_vector/5',\n",
        "        trainable=False),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(15, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "# Build the model with the default input shape\n",
        "m.build((None, 224, 224, 3))  # Batch input shape.\n",
        "\n",
        "# Print the model summary\n",
        "print(m.summary())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoOXKavG0f5w",
        "outputId": "ba93d7e6-d8ee-47c4-cb79-3c070e1d2b59"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " keras_layer_1 (KerasLayer)  (None, 1024)              1026552   \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 64)                65600     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 15)                975       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,093,127\n",
            "Trainable params: 66,575\n",
            "Non-trainable params: 1,026,552\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "m.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Calculate the steps per epoch based on the desired batch size and the size of your dataset\n",
        "steps_per_epoch = len(train_generator) // batch_size\n",
        "\n",
        "# Update the number of epochs and steps_per_epoch\n",
        "m.fit(train_generator, epochs=15, steps_per_epoch=275)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPB9ZuCv0f9M",
        "outputId": "affb6e4a-1a35-4b44-e7eb-5a9a6d616293"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "275/275 [==============================] - 3544s 13s/step - loss: 2.4865 - accuracy: 0.1989\n",
            "Epoch 2/15\n",
            "275/275 [==============================] - 255s 926ms/step - loss: 1.9943 - accuracy: 0.3713\n",
            "Epoch 3/15\n",
            "275/275 [==============================] - 259s 940ms/step - loss: 1.7593 - accuracy: 0.4332\n",
            "Epoch 4/15\n",
            "275/275 [==============================] - 252s 916ms/step - loss: 1.6414 - accuracy: 0.4751\n",
            "Epoch 5/15\n",
            "275/275 [==============================] - 251s 910ms/step - loss: 1.5605 - accuracy: 0.4940\n",
            "Epoch 6/15\n",
            "275/275 [==============================] - 263s 956ms/step - loss: 1.4956 - accuracy: 0.5158\n",
            "Epoch 7/15\n",
            "275/275 [==============================] - 254s 921ms/step - loss: 1.4484 - accuracy: 0.5323\n",
            "Epoch 8/15\n",
            "275/275 [==============================] - 261s 946ms/step - loss: 1.4072 - accuracy: 0.5445\n",
            "Epoch 9/15\n",
            "275/275 [==============================] - 251s 912ms/step - loss: 1.3775 - accuracy: 0.5548\n",
            "Epoch 10/15\n",
            "275/275 [==============================] - 257s 935ms/step - loss: 1.3453 - accuracy: 0.5657\n",
            "Epoch 11/15\n",
            "275/275 [==============================] - 256s 929ms/step - loss: 1.3176 - accuracy: 0.5721\n",
            "Epoch 12/15\n",
            "275/275 [==============================] - 253s 918ms/step - loss: 1.2895 - accuracy: 0.5789\n",
            "Epoch 13/15\n",
            "275/275 [==============================] - 245s 892ms/step - loss: 1.2661 - accuracy: 0.5866\n",
            "Epoch 14/15\n",
            "275/275 [==============================] - 253s 920ms/step - loss: 1.2453 - accuracy: 0.5975\n",
            "Epoch 15/15\n",
            "275/275 [==============================] - 254s 922ms/step - loss: 1.2291 - accuracy: 0.5987\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5c2547d3c0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Evaluate the model on the test generator\n",
        "test_loss, test_accuracy = m.evaluate(test_generator)\n",
        "\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "id": "zKNz53dO1BVj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "556175dd-aa3e-4c83-94f4-9f3f44b7da1a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40/40 [==============================] - 602s 15s/step - loss: 1.1359 - accuracy: 0.6302\n",
            "Test Loss: 1.13593590259552\n",
            "Test Accuracy: 0.6301587224006653\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-g6NRo29bCtI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define the target image size and batch size\n",
        "target_size = (224, 224)  # Adjust according to your requirements\n",
        "batch_size = 64\n",
        "\n",
        "# Create ImageDataGenerators with preprocessing options\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255.0,  # Normalize pixel values between 0 and 1\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
        "\n",
        "# Create train and test generators\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    dataframe=train_data,\n",
        "    directory='/content/gdrive/MyDrive/ML/Human Action Recognition/Human Action Recognition/FINAL_TRAIN',\n",
        "    x_col='filename',  # Column name for the filenames\n",
        "    y_col='label',  # Column name for the labels\n",
        "    target_size=target_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_dataframe(\n",
        "    dataframe=test_data,\n",
        "    directory='/content/gdrive/MyDrive/ML/Human Action Recognition/Human Action Recognition/test',\n",
        "    x_col='filename',  # Column name for the filenames\n",
        "    y_col='label',  # Column name for the labels\n",
        "    target_size=target_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Define the MobileNet V3 Large model\n",
        "mobilenet_model = tf.keras.Sequential([\n",
        "    hub.KerasLayer('https://tfhub.dev/google/imagenet/mobilenet_v3_large_075_224/feature_vector/5', trainable=False),\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(15, activation='softmax')\n",
        "])\n",
        "\n",
        "# Define the EfficientNet Lite3 model\n",
        "efficientnet_model = tf.keras.Sequential([\n",
        "    hub.KerasLayer('https://tfhub.dev/tensorflow/efficientnet/lite3/classification/2', trainable=False),\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(15, activation='softmax')\n",
        "])\n",
        "\n",
        "# Create an input layer to accept the images\n",
        "input_layer = Input(shape=(target_size[0], target_size[1], 3))\n",
        "\n",
        "# Get the output predictions from both models\n",
        "mobilenet_output = mobilenet_model(input_layer)\n",
        "efficientnet_output = efficientnet_model(input_layer)\n",
        "\n",
        "# Average the predictions from both models\n",
        "ensemble_output = tf.keras.layers.Average()([mobilenet_output, efficientnet_output])\n",
        "\n",
        "# Create the ensemble model\n",
        "ensemble_model = Model(inputs=input_layer, outputs=ensemble_output)\n",
        "\n",
        "#\n"
      ],
      "metadata": {
        "id": "-dusdTcXbCyj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95cd023d-de90-4c97-dd97-f23da68635c7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 17566 validated image filenames belonging to 15 classes.\n",
            "Found 2520 validated image filenames belonging to 15 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "ensemble_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "print(ensemble_model.summary())\n"
      ],
      "metadata": {
        "id": "BUqkMiqdbC2N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9045b8f-e9c3-44d0-cc26-568da728a2f6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " sequential_2 (Sequential)      (None, 15)           3076975     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " sequential_3 (Sequential)      (None, 15)           8547447     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " average (Average)              (None, 15)           0           ['sequential_2[0][0]',           \n",
            "                                                                  'sequential_3[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 11,624,422\n",
            "Trainable params: 619,038\n",
            "Non-trainable params: 11,005,384\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Train the model\n",
        "ensemble_model.fit(train_generator, epochs=15, steps_per_epoch=275)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "acyUfZzobC6R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "16cfbae5-6274-4a6f-d450-b44325cf1206"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "275/275 [==============================] - 2121s 8s/step - loss: 1.6570 - accuracy: 0.4695\n",
            "Epoch 2/15\n",
            "275/275 [==============================] - 2144s 8s/step - loss: 1.2228 - accuracy: 0.6095\n",
            "Epoch 3/15\n",
            "275/275 [==============================] - 2167s 8s/step - loss: 1.0597 - accuracy: 0.6618\n",
            "Epoch 4/15\n",
            "275/275 [==============================] - 2150s 8s/step - loss: 0.9463 - accuracy: 0.6903\n",
            "Epoch 5/15\n",
            "275/275 [==============================] - 2156s 8s/step - loss: 0.8559 - accuracy: 0.7175\n",
            "Epoch 6/15\n",
            "275/275 [==============================] - 2177s 8s/step - loss: 0.7681 - accuracy: 0.7431\n",
            "Epoch 7/15\n",
            "275/275 [==============================] - 2167s 8s/step - loss: 0.7113 - accuracy: 0.7614\n",
            "Epoch 8/15\n",
            "275/275 [==============================] - 2165s 8s/step - loss: 0.6500 - accuracy: 0.7848\n",
            "Epoch 9/15\n",
            "275/275 [==============================] - 2171s 8s/step - loss: 0.6071 - accuracy: 0.7959\n",
            "Epoch 10/15\n",
            "275/275 [==============================] - 2161s 8s/step - loss: 0.5696 - accuracy: 0.8079\n",
            "Epoch 11/15\n",
            "275/275 [==============================] - 2171s 8s/step - loss: 0.5267 - accuracy: 0.8243\n",
            "Epoch 12/15\n",
            "275/275 [==============================] - 2173s 8s/step - loss: 0.4976 - accuracy: 0.8275\n",
            "Epoch 13/15\n",
            "184/275 [===================>..........] - ETA: 11:50 - loss: 0.4575 - accuracy: 0.8448"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnknownError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-6f8a523d0b44>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mensemble_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m275\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnknownError\u001b[0m: Graph execution error:\n\nFileNotFoundError: [Errno 2] No such file or directory: '/content/gdrive/MyDrive/ML/Human Action Recognition/Human Action Recognition/FINAL_TRAIN/Image_246(1).jpg'\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/script_ops.py\", line 267, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/engine/data_adapter.py\", line 902, in wrapped_generator\n    for data in generator_fn():\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/engine/data_adapter.py\", line 1049, in generator_fn\n    yield x[i]\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/preprocessing/image.py\", line 116, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/preprocessing/image.py\", line 370, in _get_batches_of_transformed_samples\n    img = image_utils.load_img(\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/utils/image_utils.py\", line 422, in load_img\n    with open(path, \"rb\") as f:\n\nFileNotFoundError: [Errno 2] No such file or directory: '/content/gdrive/MyDrive/ML/Human Action Recognition/Human Action Recognition/FINAL_TRAIN/Image_246(1).jpg'\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_function_98710]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "# Evaluate the model on the test generator\n",
        "test_loss, test_accuracy = ensemble_model.evaluate(test_generator)\n",
        "\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "id": "nHhmsK_KbDDb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8eb24439-3c02-4e79-98f7-f694cdb5c031"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40/40 [==============================] - 314s 8s/step - loss: 1.1436 - accuracy: 0.7123\n",
            "Test Loss: 1.143625259399414\n",
            "Test Accuracy: 0.7123016119003296\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_model.save_weights('Ensemble.h5')\n",
        "from google.colab import files\n",
        "files.download('Ensemble.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "wIW1l22o_iFD",
        "outputId": "9ab66586-81e1-4c02-cd7e-d8a67baef267"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ea2050e5-bd20-41e9-b44d-33b29fed5460\", \"Ensemble.h5\", 47077944)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def get_file_size(file_path):\n",
        "    size = os.path.getsize(file_path)\n",
        "    return size"
      ],
      "metadata": {
        "id": "0WRyTDQXdJTb"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def convert_bytes(size, unit=None):\n",
        "    if unit == \"KB\":\n",
        "        return print('File size: ' + str(round(size / 1024, 3)) + ' Kilobytes')\n",
        "    elif unit == \"MB\":\n",
        "        return print('File size: ' + str(round(size / (1024 * 1024), 3)) + ' Megabytes')\n",
        "    else:\n",
        "        return print('File size: ' + str(size) + ' bytes')"
      ],
      "metadata": {
        "id": "8bxCcDeCdJ7S"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TF_LITE_MODEL_FILE_NAME = \"tf_lite_model.tflite\""
      ],
      "metadata": {
        "id": "KTB6JaAbddnn"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_lite_converter = tf.lite.TFLiteConverter.from_keras_model(ensemble_model)\n",
        "tf_lite_converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
        "# tf_lite_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# tf_lite_converter.target_spec.supported_types = [tf.float16]\n",
        "tflite_model = tf_lite_converter.convert()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OciZYgMBIj2h",
        "outputId": "c41845a0-319e-46b2-a155-4a8134953c6a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Optimization option OPTIMIZE_FOR_SIZE is deprecated, please use optimizations=[Optimize.DEFAULT] instead.\n",
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n",
            "WARNING:absl:Optimization option OPTIMIZE_FOR_SIZE is deprecated, please use optimizations=[Optimize.DEFAULT] instead.\n",
            "WARNING:absl:Optimization option OPTIMIZE_FOR_SIZE is deprecated, please use optimizations=[Optimize.DEFAULT] instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tflite_model_name = TF_LITE_MODEL_FILE_NAME\n",
        "open(tflite_model_name, \"wb\").write(tflite_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBPy2h8dIkBt",
        "outputId": "ef856e56-9a3f-42fe-f2b6-8c72dae39c0a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12454184"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convert_bytes(get_file_size(TF_LITE_MODEL_FILE_NAME), \"MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gydL5jvGIEfR",
        "outputId": "65b88ab9-8513-42f6-a23f-6d06bafeb833"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File size: 11.877 Megabytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "azQ-xZtCIv4v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}