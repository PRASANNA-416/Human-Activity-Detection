{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd33da65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.layers import DepthwiseConv2D\n",
    "from tensorflow.keras.layers import Add\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, ReLU, Reshape, GlobalAveragePooling2D, Activation,UpSampling2D, AveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "from mediapipe.python.solutions import pose as mp_pose\n",
    "import cv2\n",
    "pose_tracker = mp_pose.Pose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56090cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10554 images belonging to 15 classes.\n",
      "Found 2990 images belonging to 15 classes.\n",
      "Found 1525 images belonging to 15 classes.\n"
     ]
    }
   ],
   "source": [
    "train_path = 'C:/Users/Prasanna P M/Human Project/ML_aug/Human Action Recognition/TRAIN_GRAY_landmarks/'\n",
    "valid_path = 'C:/Users/Prasanna P M/Human Project/ML_aug/Human Action Recognition/VALID_GRAY_landmarks/'\n",
    "test_path = 'C:/Users/Prasanna P M/Human Project/ML_aug/Human Action Recognition/TEST_GRAY_landmarks/'\n",
    "\n",
    "train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input).flow_from_directory(\n",
    "    directory=train_path, target_size=(224,224), batch_size=32)\n",
    "\n",
    "valid_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input).flow_from_directory(\n",
    "    directory=valid_path, target_size=(224, 224), batch_size=32)\n",
    "\n",
    "test_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input).flow_from_directory(\n",
    "    directory=test_path, target_size=(224,224), batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b63d82ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10554 images belonging to 15 classes.\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 11.8 GiB for an array with shape (10554, 50176, 3) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\PRASAN~1\\AppData\\Local\\Temp/ipykernel_11936/647290675.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;31m# Generate batches of image data and landmarks from the directory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m train_images, train_landmarks, train_labels = flow_from_directory_with_landmarks(\n\u001b[0m\u001b[0;32m     87\u001b[0m     \u001b[0mtrain_batches\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[0mdirectory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\PRASAN~1\\AppData\\Local\\Temp/ipykernel_11936/647290675.py\u001b[0m in \u001b[0;36mflow_from_directory_with_landmarks\u001b[1;34m(generator, directory, target_size, color_mode, classes, class_mode, batch_size, shuffle, seed, save_to_dir, save_prefix, save_format, follow_links, subset, interpolation)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[1;31m# Append zeros and reshape landmarks array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m     \u001b[0mlandmarks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlandmarks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50176\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m33\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m     \u001b[0mlandmarks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlandmarks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 11.8 GiB for an array with shape (10554, 50176, 3) and data type float64"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from mediapipe.python.solutions import pose as mp_pose\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def flow_from_directory_with_landmarks(\n",
    "    generator,\n",
    "    directory,\n",
    "    target_size=(224, 224),\n",
    "    color_mode=\"rgb\",\n",
    "    classes=None,\n",
    "    class_mode=\"categorical\",\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    seed=None,\n",
    "    save_to_dir=None,\n",
    "    save_prefix=\"\",\n",
    "    save_format=\"png\",\n",
    "    follow_links=False,\n",
    "    subset=None,\n",
    "    interpolation=\"nearest\",\n",
    "):\n",
    "    # Get the original flow_from_directory output\n",
    "    iterator = generator.flow_from_directory(\n",
    "        directory,\n",
    "        target_size=target_size,\n",
    "        color_mode=color_mode,\n",
    "        classes=classes,\n",
    "        class_mode=class_mode,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        seed=seed,\n",
    "        save_to_dir=save_to_dir,\n",
    "        save_prefix=save_prefix,\n",
    "        save_format=save_format,\n",
    "        follow_links=follow_links,\n",
    "        subset=subset,\n",
    "        interpolation=interpolation,\n",
    "    )\n",
    "\n",
    "    # Create empty lists to store image data and landmarks\n",
    "    image_data = []\n",
    "    landmarks = []\n",
    "\n",
    "    # Load and preprocess each image with landmarks\n",
    "    pose_tracker = mp_pose.Pose()\n",
    "    for image_path in iterator.filepaths:\n",
    "        # Read the image\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        # Extract landmarks using mediapipe.pose\n",
    "        results = pose_tracker.process(image)\n",
    "        pose_landmarks = results.pose_landmarks\n",
    "\n",
    "        # Check if landmarks were detected\n",
    "        if pose_landmarks is not None:\n",
    "            # Extract landmark coordinates\n",
    "            landmark_coordinates = [[lmk.x, lmk.y, 0.0] for lmk in pose_landmarks.landmark]\n",
    "            landmarks.append(landmark_coordinates)\n",
    "        else:\n",
    "            # If landmarks were not detected, append zeros\n",
    "            landmarks.append(np.zeros((33, 3)))\n",
    "\n",
    "        # Preprocess and resize the image\n",
    "        image = cv2.resize(image, target_size)\n",
    "        image_data.append(image)\n",
    "\n",
    "    # Convert the image data and landmarks to numpy arrays\n",
    "    image_data = np.array(image_data)\n",
    "    landmarks = np.array(landmarks)\n",
    "\n",
    "    # Append zeros and reshape landmarks array\n",
    "    landmarks = np.concatenate((landmarks, np.zeros((len(image_data), 50176 - 33, 3))), axis=1)\n",
    "    landmarks = landmarks.reshape((len(image_data), 224, 224, 3))\n",
    "\n",
    "    # Return the modified image data and labels\n",
    "    return image_data, landmarks, iterator.labels\n",
    "\n",
    "\n",
    "# Usage example\n",
    "train_path = 'C:/Users/Prasanna P M/Human Project/ML_aug/Human Action Recognition/TRAIN_GRAY_landmarks/'\n",
    "train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input)\n",
    "\n",
    "# Generate batches of image data and landmarks from the directory\n",
    "train_images, train_landmarks, train_labels = flow_from_directory_with_landmarks(\n",
    "    train_batches,\n",
    "    directory=train_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Verify the shapes of the data\n",
    "print(train_images.shape)       # Shape of train_images\n",
    "print(train_landmarks.shape)    # Shape of train_landmarks\n",
    "print(train_labels.shape)       # Shape of train_labels\n",
    "\n",
    "# Convert training labels to categorical format\n",
    "train_labels_encoded = tf.keras.utils.to_categorical(train_labels, num_classes=15)\n",
    "print(train_labels_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f9f0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the variable names and paths for validation data\n",
    "val_path = 'C:/Users/Prasanna P M/Human Project/ML_aug/Human Action Recognition/VALID_GRAY_landmarks/'\n",
    "val_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input)\n",
    "\n",
    "# Generate batches of image data and landmarks from the validation directory\n",
    "val_images, val_landmarks, val_labels = flow_from_directory_with_landmarks(\n",
    "    val_batches,\n",
    "    directory=val_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Verify the shapes of the validation data\n",
    "print(val_images.shape)       # Shape of val_images\n",
    "print(val_landmarks.shape)    # Shape of val_landmarks\n",
    "print(val_labels.shape)       # Shape of val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41883c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_labels_encoded = tf.keras.utils.to_categorical(val_labels, num_classes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e2f7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "from mediapipe.python.solutions import pose as mp_pose\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0558e6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dropout, Dense, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "model1 = tf.keras.Sequential([\n",
    "    hub.KerasLayer(\n",
    "        'https://tfhub.dev/google/imagenet/mobilenet_v3_large_100_224/feature_vector/5',\n",
    "        trainable=True),\n",
    "    tf.keras.layers.Dropout(0.2)\n",
    "\n",
    "])\n",
    "\n",
    "# Build the model\n",
    "model1.build((None, 224, 224, 3))\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f6929a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tf.keras.Sequential([\n",
    "    hub.KerasLayer(\n",
    "        'https://tfhub.dev/google/imagenet/mobilenet_v3_large_100_224/feature_vector/5',\n",
    "        trainable=True),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "])\n",
    "\n",
    "# Build the model\n",
    "model2.build((None, 224, 224, 3))\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869dceee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170c6d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = concatenate([model1.output,model2.output])\n",
    "prediction = Dense(15, activation='softmax')(combined)\n",
    "\n",
    "fin_model = Model(inputs=[model1.input,model2.input], outputs=prediction)\n",
    "fin_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39c9d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636614bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "fin_model.fit(\n",
    "    [train_landmarks,train_images],  \n",
    "    train_labels_encoded,                     \n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=([val_landmarks,val_images], valid_labels_encoded),\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c47d7cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c8019b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22805e59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5348470b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29888df8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04d7885c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10554 images belonging to 15 classes.\n",
      "(10554, 224, 224, 3)\n",
      "(10554, 32, 32, 3)\n",
      "(10554,)\n",
      "(10554, 15)\n"
     ]
    }
   ],
   "source": [
    "train_path = 'C:/Users/Prasanna P M/Human Project/ML_aug/Human Action Recognition/TRAIN_landmarks/'\n",
    "train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input)\n",
    "\n",
    "# Generate batches of image data and landmarks from the directory\n",
    "train_images, train_landmarks, train_labels = flow_from_directory_with_landmarks(\n",
    "    train_batches,\n",
    "    directory=train_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Verify the shapes of the data\n",
    "print(train_images.shape)       # Shape of train_images\n",
    "print(train_landmarks.shape)    # Shape of train_landmarks\n",
    "print(train_labels.shape)       # Shape of train_labels\n",
    "\n",
    "# Convert training labels to categorical format\n",
    "train_labels_encoded = tf.keras.utils.to_categorical(train_labels, num_classes=15)\n",
    "print(train_labels_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37fb171d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2990 images belonging to 15 classes.\n",
      "(2990, 224, 224, 3)\n",
      "(2990, 32, 32, 3)\n",
      "(2990,)\n"
     ]
    }
   ],
   "source": [
    "# Modify the variable names and paths for validation data\n",
    "val_path = 'C:/Users/Prasanna P M/Human Project/ML_aug/Human Action Recognition/VALID_landmarks/'\n",
    "val_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input)\n",
    "\n",
    "# Generate batches of image data and landmarks from the validation directory\n",
    "val_images, val_landmarks, val_labels = flow_from_directory_with_landmarks(\n",
    "    val_batches,\n",
    "    directory=val_path,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Verify the shapes of the validation data\n",
    "print(val_images.shape)       # Shape of val_images\n",
    "print(val_landmarks.shape)    # Shape of val_landmarks\n",
    "print(val_labels.shape)       # Shape of val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "deefcb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_labels_encoded = tf.keras.utils.to_categorical(val_labels, num_classes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92d2e63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f56fb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "330/330 [==============================] - 1705s 5s/step - loss: 0.5749 - accuracy: 0.8954 - val_loss: 1.7146 - val_accuracy: 0.6445\n",
      "Epoch 2/10\n",
      "330/330 [==============================] - 1710s 5s/step - loss: 0.3562 - accuracy: 0.9686 - val_loss: 1.7966 - val_accuracy: 0.6629\n",
      "Epoch 3/10\n",
      "  1/330 [..............................] - ETA: 26:49 - loss: 0.2753 - accuracy: 1.0000"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "fin_model.fit(\n",
    "    [train_landmarks,train_images],  \n",
    "    train_labels_encoded,                     \n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=([val_landmarks,val_images], valid_labels_encoded),\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33580f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
