{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PRASANNA-416/Human-Activity-Detection-/blob/main/Human_Activity_Detection_MobileNet_v3_small_EXPERIMENTING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RBku0wg6YAC",
        "outputId": "edf002d9-b58f-4b44-d034-6484d7c9283c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Z8voouPk6bsD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the training CSV file\n",
        "train_csv_path = '/content/gdrive/MyDrive/ML/Human Action Recognition/Training_set.csv'\n",
        "train_data = pd.read_csv(train_csv_path)\n",
        "\n",
        "# Load the testing CSV file\n",
        "test_csv_path = '/content/gdrive/MyDrive/ML/Human Action Recognition/testing_set.csv'\n",
        "test_data = pd.read_csv(test_csv_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPpjJySU6bu2",
        "outputId": "9928b960-678f-4caa-f142-58fc2e0fc309"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 12100 validated image filenames belonging to 15 classes.\n",
            "Found 500 validated image filenames belonging to 15 classes.\n",
            "{'calling': 0, 'clapping': 1, 'cycling': 2, 'dancing': 3, 'drinking': 4, 'eating': 5, 'fighting': 6, 'hugging': 7, 'laughing': 8, 'listening_to_music': 9, 'running': 10, 'sitting': 11, 'sleeping': 12, 'texting': 13, 'using_laptop': 14}\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Load the CSV files\n",
        "train_csv_path = '/content/gdrive/MyDrive/ML/Human Action Recognition/Training_set.csv'\n",
        "test_csv_path = '/content/gdrive/MyDrive/ML/Human Action Recognition/testing_set.csv'\n",
        "\n",
        "train_data = pd.read_csv(train_csv_path)\n",
        "test_data = pd.read_csv(test_csv_path)\n",
        "\n",
        "# Define the target image size and batch size\n",
        "target_size = (224, 224)  # Adjust according to your requirements\n",
        "batch_size = 64\n",
        "\n",
        "# Create ImageDataGenerators with preprocessing options\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255.0,  # Normalize pixel values between 0 and 1\n",
        "    # rotation_range=20,  # Randomly rotate images by 20 degrees\n",
        "    # width_shift_range=0.2,  # Randomly shift the width by 20%\n",
        "    # height_shift_range=0.2,  # Randomly shift the height by 20%\n",
        "    # horizontal_flip=True  # Randomly flip images horizontally\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
        "\n",
        "# Create train and test generators\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    dataframe=train_data,\n",
        "    directory='/content/gdrive/MyDrive/ML/Human Action Recognition/train',  # Directory containing the train images\n",
        "    x_col='filename',  # Column name for the filenames\n",
        "    y_col='label',  # Column name for the labels\n",
        "    target_size=target_size,\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_dataframe(\n",
        "    dataframe=test_data,\n",
        "    directory='/content/gdrive/MyDrive/ML/Human Action Recognition/test',\n",
        "    x_col='filename',  # Column name for the filenames\n",
        "    y_col='label',  # Column name for the labels\n",
        "    target_size=target_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Check the class indices assigned by the generator\n",
        "class_indices = test_generator.class_indices\n",
        "print(class_indices)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFkpCbAj6bxu",
        "outputId": "4db65f20-2c73-49c8-a64f-3c2958dee4fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " keras_layer_3 (KerasLayer)  (None, 1024)              1026552   \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 256)               262400    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 32)                4128      \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 32)                0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 15)                495       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,326,471\n",
            "Trainable params: 1,316,791\n",
            "Non-trainable params: 9,680\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    hub.KerasLayer(\n",
        "        'https://tfhub.dev/google/imagenet/mobilenet_v3_small_075_224/feature_vector/5',\n",
        "        trainable=True),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(15, activation='softmax')\n",
        "])\n",
        "#https://tfhub.dev/tensorflow/efficientnet/lite3/classification/2\n",
        "model.build((None, 224, 224, 3)) \n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SHsuZg-C6b0i",
        "outputId": "a63892c0-27c2-499c-f97e-9969a7a8bedc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "190/190 [==============================] - 701s 4s/step - loss: 2.8141 - accuracy: 0.1405\n",
            "Epoch 2/25\n",
            "190/190 [==============================] - 672s 4s/step - loss: 2.4125 - accuracy: 0.2917\n",
            "Epoch 3/25\n",
            "190/190 [==============================] - 664s 3s/step - loss: 2.1179 - accuracy: 0.4068\n",
            "Epoch 4/25\n",
            "190/190 [==============================] - 664s 3s/step - loss: 1.9109 - accuracy: 0.4739\n",
            "Epoch 5/25\n",
            "190/190 [==============================] - 679s 4s/step - loss: 1.7500 - accuracy: 0.5277\n",
            "Epoch 6/25\n",
            "190/190 [==============================] - 679s 4s/step - loss: 1.6214 - accuracy: 0.5679\n",
            "Epoch 7/25\n",
            "190/190 [==============================] - 690s 4s/step - loss: 1.5187 - accuracy: 0.6063\n",
            "Epoch 8/25\n",
            "190/190 [==============================] - 683s 4s/step - loss: 1.4047 - accuracy: 0.6425\n",
            "Epoch 9/25\n",
            "190/190 [==============================] - 677s 4s/step - loss: 1.3001 - accuracy: 0.6729\n",
            "Epoch 10/25\n",
            "190/190 [==============================] - 676s 4s/step - loss: 1.1973 - accuracy: 0.7025\n",
            "Epoch 11/25\n",
            "190/190 [==============================] - 670s 4s/step - loss: 1.1098 - accuracy: 0.7388\n",
            "Epoch 12/25\n",
            "190/190 [==============================] - 663s 3s/step - loss: 1.0458 - accuracy: 0.7578\n",
            "Epoch 13/25\n",
            "190/190 [==============================] - 659s 3s/step - loss: 0.9642 - accuracy: 0.7788\n",
            "Epoch 14/25\n",
            "190/190 [==============================] - 658s 3s/step - loss: 0.9067 - accuracy: 0.7992\n",
            "Epoch 15/25\n",
            "190/190 [==============================] - 660s 3s/step - loss: 0.8561 - accuracy: 0.8170\n",
            "Epoch 16/25\n",
            "190/190 [==============================] - 659s 3s/step - loss: 0.7895 - accuracy: 0.8383\n",
            "Epoch 17/25\n",
            "190/190 [==============================] - 662s 3s/step - loss: 0.7431 - accuracy: 0.8491\n",
            "Epoch 18/25\n",
            "190/190 [==============================] - 670s 4s/step - loss: 0.7059 - accuracy: 0.8597\n",
            "Epoch 19/25\n",
            "190/190 [==============================] - 662s 3s/step - loss: 0.6572 - accuracy: 0.8769\n",
            "Epoch 20/25\n",
            "190/190 [==============================] - 660s 3s/step - loss: 0.6066 - accuracy: 0.8916\n",
            "Epoch 21/25\n",
            "190/190 [==============================] - 664s 3s/step - loss: 0.6065 - accuracy: 0.8925\n",
            "Epoch 22/25\n",
            "190/190 [==============================] - 666s 4s/step - loss: 0.5713 - accuracy: 0.9050\n",
            "Epoch 23/25\n",
            "190/190 [==============================] - 666s 4s/step - loss: 0.5326 - accuracy: 0.9136\n",
            "Epoch 24/25\n",
            "190/190 [==============================] - 664s 3s/step - loss: 0.5080 - accuracy: 0.9190\n",
            "Epoch 25/25\n",
            "167/190 [=========================>....] - ETA: 1:19 - loss: 0.5174 - accuracy: 0.9202"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnknownError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-6a9392af7b7b>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Update the number of epochs and steps_per_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m190\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnknownError\u001b[0m: Graph execution error:\n\nFileNotFoundError: [Errno 2] No such file or directory: '/content/gdrive/MyDrive/ML/Human Action Recognition/train/Image_9384.jpg'\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/script_ops.py\", line 267, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/engine/data_adapter.py\", line 902, in wrapped_generator\n    for data in generator_fn():\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/engine/data_adapter.py\", line 1049, in generator_fn\n    yield x[i]\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/preprocessing/image.py\", line 116, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/preprocessing/image.py\", line 370, in _get_batches_of_transformed_samples\n    img = image_utils.load_img(\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/utils/image_utils.py\", line 422, in load_img\n    with open(path, \"rb\") as f:\n\nFileNotFoundError: [Errno 2] No such file or directory: '/content/gdrive/MyDrive/ML/Human Action Recognition/train/Image_9384.jpg'\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_function_121997]"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Calculate the steps per epoch based on the desired batch size and the size of your dataset\n",
        "steps_per_epoch = len(train_generator) // batch_size\n",
        "\n",
        "# Update the number of epochs and steps_per_epoch\n",
        "model.fit(train_generator, epochs=25, steps_per_epoch=190)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UC-tv35E6b3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42d1c5e5-cf02-404e-9946-a305b3a8d53f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 7s 729ms/step - loss: 1.8680 - accuracy: 0.7180\n",
            "Test Loss: 1.8680005073547363\n",
            "Test Accuracy: 0.7179999947547913\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Evaluate the model on the test generator\n",
        "test_loss, test_accuracy = model.evaluate(test_generator)\n",
        "\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "agP4Wh9x6b6A"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "model.save_weights('EfficientNetLite3.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "A5Hhu0ja6b8t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "b52cb09a-9ff8-4004-e9e4-3bc2e195cde7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b674f373-c07f-4eea-b031-ddeaaaf6e2ff\", \"EfficientNetLite3.h5\", 5518920)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "from google.colab import files\n",
        "files.download('EfficientNetLite3.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "do9WMzHy6b_d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnNv8ATg6cCV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJqACbAg6cGD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdzZ3-ZX6cIu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMV+EbsTwpsxCLwygo22GUE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}